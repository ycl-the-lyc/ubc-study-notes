#import "../lib.typ": *
#show: setup.with(
  title: [MATH 256 _Differential Equations_],
  author: "Yecheng Liang",
)

#set math.vec(delim: "[")
#set math.mat(delim: "[")

#let ll = math.op[L]

= Differential Equations
#definition(title: [Differential Equation])[
  An equation that defines a function by giving a relationship between the function and its derivative.
]

#example[
  $
    y' + y = x.
  $
]

Integration is solving a trivial differential equation.

Given a first-order, linear differential equation, $dv(y, x) = a y$, a reasonable guess is $y = A e^(a x)$.
We can take any constant $A$ because the equation is linear.

== Seperable Equations Method
For non-linear differential equations, we would manipulate the equation so one side is expressed by $y$, the other by $x$ and $dd(x)$.
Then, integrating both sides gives us a relationship between $x$ and $y$.

#example[
  $
              dv(y, x) & = a y \
             dd(y) / y & = a dd(x) \
    integral dd(y) / y & = a integral dd(x) + C \
             ln abs(y) & = a x + C \
                     y & = e^(ln(abs(y))) \
                       & = e^(a x + C) \
                       & = e^c e^(a x) \
                       & = A e^(a x).
  $
]

== Integrating Factor Method
With a linear first-order equation, we can forcibly express it by only $x$.

Let $ll$ be an operator such that
$
  ll y & = (dv(, x) + p(x)) y \
       & = dv(y, x) + p(x) y \
       & = g(x).
$
So long as $p(x), g(x)$ are continuous, a solution exists.
And, if $ll y = 0$, we say the equation is homogeneous.

Let $F(x)$ be the _integrating factor_ such that
$
  F(x) dv(y, x) + F(x) p(x) y & = dv(, x) [F(x) y].
$
From the equation above we can see that, if we differentiate $F(x)$, we get itself multiplied by $p(x)$, looks much like our guess of exponential before.
Hence,
$
  F(x) & = e^(integral p(x) dd(x)).
$

Multiply $ll y$ by $F(x)$,
$
  e^(integral p(x) dd(x)) dv(y, x) + e^(integral p(x) dd(x)) p(x) y & = e^(integral p(x) dd(x)) g(x)
$
Notice that the LHS is differential of a product.
$
  dv(, x) [e^(integral p(x) dd(x)) y] & = e^(integral p(x) dd(x)) g(x) \
            e^(integral p(x) dd(x)) y & = integral e^(integral p(x) dd(x)) g(x) dd(x) + C \
                                    y & = e^(-integral p(x) dd(x)) (integral e^(integral p(x) dd(x)) g(x) dd(x) + C).
$

#problem[
  Solve
  $
    cases(
      ll y = y' + y = x,
      y(0) = 1
    )
  $

  #solution[
    Using the integral factor form, let
    $
       p(x) & = 1 \
       y(x) & = x \
      y'(x) & = 1.
    $
    Thus, we find the integrating factor to be
    $
      F(x) & = e^(integral p(x) dd(x)) \
           & = e^x.
    $
    Then, transform the equation by multiplying $F(x) = e^x$ to both sides.
    $
      e^x y' + e^x y & = [e^x y]' \
                     & = x e^x.
    $
    Notice that the LHS is differential of a product, undo the chain rule, we get
    $
      e^x y & = e^x x - e^x + C.
    $
    Don't forget the $C$ generated by the integrating both sides.

    Move the factor of integration to the RHS,
    $
      y & = x - 1 + C e^(-x)
    $
    Plug in $y(0) = 1$,
    $
      y(x) = x - 1 + 2e^(-x).
    $
  ]
]

#problem[
  Solve
  $
    ll y & = y' + y = sin(x).
  $

  #solution[
    Recall Euler's Equation,
    $
      e^(i x) = cos(x) + i sin(x).
    $

    Similar to the previous problem,
    $
          F(x) & = e^x ll y \
               & = [e^x y]' \
               & = e^x sin(x) \
      [e^x y]' & = Im e^((1 + i)x).
    $
    Integrate both sides,
    $
      e^x y & = Im e^((1 + i)x) / (1 + i) + C.
    $
    Turn the fraction term of RHS into proper form,
    $
      e^((1 + i)x) / (1 + i) & = (e^((1 + i)x) (1 - i)) / ((1 + i) (1 - i)) \
                             & = (e^((1 + i)x) (1 - i)) / 2.
    $
    Continue with the factor of integration method,
    $
      y & = Im (e^((1 + i)x) (1 - i) e^(-x)) / 2 + C e^(-x) \
        & = Im (e^(i x) (1 - i)) / 2 + C e^(-x) \
        & = Im ((cos(x) + i sin(x)) (1 - i)) / 2 + C e^(-x) \
        & = Im ((cos(x) - i^2 sin(x)) + (i sin(x) - i cos(x))) / 2 + C e^(-x) \
        & = Im ((cos(x) + sin(x)) + i (sin(x) - cos(x))) / 2 + C e^(-x) \
        & = (sin(x) - cos(x)) / 2 + C e^(-x).
    $
    No particular solution.
  ]
]

== Undetermined Coefficient Method
For a homogeneous linear equation, we can guess a particular solution, $y_p$.
- If $g(x) = x^n$, then $y_p = a_n x^n + a_(n-1) x^(n-1) + ... + a_1 x + a_0$.
- If $g(x) = e^(a x)$, then $y_p = A e^(a x)$.
- If $g(x) = sin(x) "or" cos(x)$, then $y_p = A cos(x) + B sin(x)$.
The guesses are linear combinations of all possible derivatives of $g(x)$.

The particular solution still carries undetermined coefficients.
Assume this $y_p$ to transform $ll y$.

= Higher Order ODEs

== Linear Second-order ODEs
Redefine the operator $ll$:
$
  ll y & := y'' + p(x) y' + q(x) y = g(x).
$
If $g(x) = 0$, we say the equation is homogeneous.

Such equations are common in superposition of waves and such.

== Constant Coefficient Equations
If $g(x) = 0$ and the coefficients are constant, like
$
  ll y = a y'' + b y' + c y = 0,
$
then,
$
     y & = e^(r x) \
  ll y & = (a r^2 + b r + c) e^(r x) = 0.
$

Since $e^(r x) eq.not 0$, solving the equation is the same as solving the contained quadratic equation.
If $Delta < 0$, then the roots are complex,
$
  r_(1, 2) & = lambda plus.minus i mu \
      y(x) & = e^(r x) (A cos(mu x) + B sin(mu x))
$

#problem[
  A stone thrown vertically up at speed $v_0$ at height $h$.
  Express its displacement as a function of $t$.

  #solution[
    $
                     y'' & = -g \
      integral y'' dd(t) & = integral -g dd(t) \
                      y' & = -g t + C \
                   y'(0) & = v_0 = C \
       integral y' dd(t) & = integral -g t + v_0 dd(t) \
                       y & = v_0 t - 1/2 g t^2 + D \
                    y(0) & = h = D \
                    y(t) & = h + v_0 t - 1/2 g t^2.
    $
  ]
]

== Initial Value Problem
With similar setup to the previous problem, but the coefficients may not be constants.

Given $ll y = 0$ and initial conditions $y(x_0) = y_0, y'(x_0) = v_0$, let $y_1(x), y_2(x)$ be two solutions to $ll y = 0$.
Since $ll$ is a linear operation, its solution is also linear.
$
     y(x) & = C_1 y_1(x) + C_2 y_2(x) \
   y(x_0) & = C_1 y_1(x_0) + C_2 y_2(x_0) \
          & = y_0 \
  y'(x_0) & = C_1 y'_1(x_0) + C_2 y'_2(x_0) \
          & = v_0.
$
This can be written as
$
  underbrace(mat(y_1(x_0), y_2(x_0); y'_1(x_0), y'_2(x_0)), A) mat(C_1; C_2) = mat(y_0; v_0).
$

For it to be solved as having constant coefficients, $det(A) eq.not 0$, i.e. $A^(-1)$ must exist.

#definition(title: [Wronsian Function])[
  Given $y_1(x), y_2(x)$,
  $
    W(y_1, y_2)(x) := mdet(y_1(x), y_2(x); y'_1(x), y'_2(x)).
  $
]

#definition(title: [Fundamental Set of Solutions])[
  Given $ll y = y'' + p(x) y' + q(x) y = 0$, the pair ${y_1(x), y_2(x)}$ is a set of fundamental solutions on an interval $I$, provided that $W(Y_1, y_2)(x_0)$ where $x_0 in I$.
]

So, an initial value problem has solution if $W(y_1, y_2)(x_0) eq.not 0$.

== Linear Dependency of Functions
Given two functions $f(x), g(x)$ which are valid on interval $I$, they are linearly dependent on $I$ if there exists non-zero $c_1, c_2$ such that
$
  c_1 y(x) + c_2 g(x) = 0.
$

If $y(x) = C_1 y_1(x) + C_2 y_2(x)$, then ${y_1, y_2}$ is a set of fundamental solutions.

== Simple Harmonic Motion
Second-order differential equations are common in problems where contributing factors also produce hindrances, like spring-mass and inductor-capacitor systems.

#example[
  Given a spring hanging a mass,
  $
    m dot.double(x) & = - m g - k x - beta dot(x) \
    m dot.double(x) + underbrace(beta dot(x), #[damping]) + underbrace(k x, #[energy\ storage]) & = underbrace(- m g, #[forcing]).
  $
]

When $beta = 0$, it is without damping, thus simple harmonic motion.

=== Free Undamped Oscillation
Ignore external influence ("free") and damping.

#example[
  $
                              g & = 0 \
                           beta & = 0 \
          m dot.double(x) + k x & = 0 \
                              x & = e^(r t) \
                        omega_0 & = sqrt(k/m) \
    dot.double(x) + omega_0^2 x & = 0 \
      (r^2 + omega_0^2) e^(r t) & = 0 \
                              r & = plus.minus i omega_0 \
                           y(t) & = A cos(omega_0 t) + B sin(omega_0 t) \
                                & = R cos(omega_0 t - phi) quad ("double angle") \
                                & = R cos(omega_0 t) cos(phi) + R sin(omega_0 t) sin(phi) \
                              A & = R cos(phi) \
                              B & = R sin(phi) \
                              R & = sqrt(A^2 + B^2) \
                          B / A & = tan(phi) \
                            phi & = tan^(-1)(B / A)
  $
]

=== Free Damped Oscillation
There is still no external forcing.

#example[
  $
                                      g & = 0 \
                                   beta & > 0 \
    m dot.double(x) + beta dot(x) + k x & = 0.
  $

  See this as a constant coefficient equation,
  $
                  x(t) & = e^(r t) \
    m r^2 + beta r + k & = 0 \
                 Delta & = beta^2 + 4 m k \
                     r & = (-beta plus.minus sqrt(Delta)) / (2m).
  $

  If $Delta < 0$, then
  $
    r_(1, 2) & = - beta / (2m) plus.minus i sqrt(4 m k - beta^2) / (2m) \
             & = - beta / (2m) plus.minus omega_1
  $
  where
  $
    omega_1 & = sqrt((4 m k - beta^2) / (4m^2)) \
            & = sqrt(k/m - beta^2 / (4m^2)) \
            & = sqrt(k/m (1 - beta^2 / (4 m k))) \
            & = sqrt(k/m) sqrt(1 - beta^2 / (4 m k)) \
            & = omega_0 sqrt(1 - beta^2 / (4 m k)).
  $

  Substitute it back, plus Euler's equation,
  $
    x(t) = e^(- beta / (2m) t) [A cos(omega_1) t + B sin(omega_1) t]
  $
  which is an underdamp.

  If $Delta > 0$, then solve for $r_1, r_2$ as usual.
  Since $beta, m, k > 0$, the roots will be negative,
  $
    x(t) = A e^(r_1 t) + B e^(r_2 t)
  $
  which is an overdamp.

  If $Delta = 0$, then
  $
    beta^2 - 4 m k & = 0 \
              beta & = 2 sqrt(m k) \
                 r & = - beta / (2 m k) \
              x(t) & = e^(- beta / (2m)) [A + B t]
  $
  which is a critcal damp.
]

== Inhomogeneous Second-order Equations
Define
$
  ll y & = y'' + p(x) y' + q(x) y = g(x)
$
where $g(x) eq.not 0$.

It has the general solution of
$
  y(x) & = y_P (x) + y_H (x)
$
where $ll y_P (x) = g(x), ll y_H (x) = 0$.

If $g(x) = 0$, then $y_P$ disappears, then we can solve the constant coefficient cases as usual.
$
              y_H & = e^(r x) \
  a r^2 + b r + c & = 0 \
                r & = ... \
             y(x) & = y_H (x) = c_1 y_1 (x) + c_2 y_2 (x).
$

To find $y_P$, we guess it to be a linear combination of all possible derivatives of $g(x)$.
- $x^n$: $y_P = a_n x^n + a_(n-1) x^(n-1) + ... + a_1 x + a_0$.
- $e^(a x)$: $y_P = A e^(a x)$.
- $sin(omega x), cos(omega x)$: $y_P = A cos(omega x) + B sin(omega x)$.

#note-box[
  This does not work when one or more terms of $y_P$ is a solution to the homogeneous equation.
  To make it work, we multiply the guess of $y_P$ by one or more $x$, until the highest power of $ll y_P$ matches the ones in $g(x)$.
]

#problem[
  Solve $ll y = y'' + 3y' + 2y = e^x$.

  #solution[
    Guess $y_P = A e^x$.
    Solve for $y_H$ (omitted).
    $
       ll y_P & = A e^x + 3 A e^x + 2 A e^x = e^x \
      6 A e^x & = e^x \
            A & = 1/6.
    $
    Combine $y_P, y_H$,
    $
      y(x) & = 1/6 e^x + c_1 e^(-x) + c_2 e^(-2 x).
    $
  ]
]

== Forced Oscillations and Resonnance
For all periodic motions, we can write them into some
$
  m x'' + c x' + k x = F_0 cos(omega t).
$

=== Undamped Forced Motion and Resonnance
Since it is undamped, $c = 0$.

Recall that our spring-mass system has the homogeneous solution of
$
  x_H = C_1 cos(omega_0 t) + C_2 sin(omega_0 t)
$
where $omega_0 = sqrt(k/m)$ is the _natural frequency_.

If $omega eq.not omega_0$, we know $x_p = A cos(omega t)$ where $A$ is some constant.
Solve for $x_p$,
$
  x_P = F_0 / (m (omega_0^2 - omega^2)) cos(omega t).
$

The general solution is thus the linear combination of $x_h$ and $x_p$:
$
  x = C_1 cos(omega_0 t) + C_2 sin(omega_0 t) + F_0 / (m (omega_0^2 - omega^2)) cos(omega t).
$

=== Damped Forced Motion and Partial Resonnance
Let
$
        p & = c / (2m) \
  omega_0 & = sqrt(k/m).
$
Then,
$
  ll x_P = & dot.double(x_P) + beta / m dot(x_P) + k / m x_P = F_0 / m cos(omega t) \
         = & -omega^2 [A cos(omega t) + B sin(omega t)] \
           & + k / m [-A omega sin(omega t) + B omega cos(omega t)] \
           & + [A cos(omega t) + B sin(omega t)] \
         = & (omega_0^2 - omega^2) [A cos(omega t) + B sin(omega t)] \
           & + beta / m [-A omega sin(omega t) + B omega cos(omega t)] \
         = & cos(omega t) [A (omega_0^2 - omega^2) + B (omega beta) / m] \
           & + sin(omega t) [B (omega_0^2 - omega^2) - A (omega beta) / m].
$

Equate the first and last line, we know that the $sin... = 0, cos... = F_0 / m$.
So solve for $A, B$ (omitted).

Let $R = sqrt(A^2 + B^2)$, we can combine the $sin$ and $cos$ terms
$
  x_P (t) = & R cos(omega t - phi)
$
where $phi = tan^-1 (((omega beta) / m) / (omega_0^2 - omega^2))$.

== Cauchy-Euler Equation
Define
$
  ll y = & a x^2 y'' + b x y' + c y = 0
$
where $a, b, c$ are constants.

If $a = 0$, the equation becomes first-order and seperable.
$
  ln abs(y) = & - c / b ln abs(x) + D \
       y(x) = & e^(ln abs(y)) \
            = & e^(ln abs(x))^(-c/b) e^D \
            = & A x^(-c/b).
$

For the second-order equation, we guess that
$
        y = & x^r \
       y' = & r x^(r - 1) \
      y'' = & r (r - 1) x^(r - 2) \
     x y' = & r x^r \
  x^2 y'' = & r (r - 1) x^r.
$

Apply $ll$ on this guess: for all $x$,
$
               ll x^r = & [a r (r - 1) + b r + c] x^r \
                        & = 0 \
  a r^2 + (b - a) r + c & = 0.
$

Then solve it in similar fasion to the previous second-order equations.
- If $Delta > 0$,
  $
    y(x) = & C_1 x^(r_1) + C_2 x^(r_2).
  $
- If $Delta < 0$,
  $
    r_(1, 2) = & - (b - a) / (2 a) plus.minus ((-1) (4 a c - (b - a)^2)) \
             = & alpha plus.minus i beta
  $
  where $alpha, beta$ are obvious.
  $
    y(x) = & C_1 x^(alpha + i beta) + C_2 x^(alpha - i beta) \
         = & x^alpha [C_1 e^(i beta ln(x)) + C_2 e^(-i beta ln(x))] \
         = & x^alpha [A cos(beta ln(x)) + B sin(beta ln(x))].
  $
- If $Delta = 0$, the one repeated root $r = - (b - a) / (2 a)$.
  One solution is
  $
    y_1 (x) = x^r.
  $
  The other can be found via
  $
    y_2 (x) = & evaluated(pdv(, r) x^r)_(r = r_1) \
            = & evaluated(pdv(, r) e^(r ln(x)))_(r = r_1) \
            = & x^r ln(x).
  $
  This method is explained in the next section.
  $y(x)$ is then a linear combination of the two solutions.

// TODO
// $
//   oL x^r = & [a x^2 dv(, x, 2) + b x dv(, x) + c] x^r \
//          = & [a r^2 + (b - a) r + c] x^2 \
//          = & a [(r + (b - a) / (2a))^2 - overbrace((b - a)^2 - 4 a c, Delta = 0) / (4 a^2)] x^2 \
//          = & a (r + (b - a) / (2a))^2 x^r
// $
//
// $
//   oL(evaluated(pdv(x^r, r))_(r = 1)) = & 0.
// $

== Reduction of Order Method
Given the second-order differential equation.
If $y_1 (x)$ is a solution to the homogeneous equation, then the other solution $y_2 (x)$ can be found in form of
$
    y_2 (x) = & u(x) y_1 (x) \
   y'_2 (x) = & u(x) y'_1 (x) + u' (x) + y_1 (x) \
  y''_2 (x) = & u(x) y''_1 (x) + 2 u' (x) y'_1 (x) + u'' (x) + y_1 (x).
$
Since we know $ll y_1 = 0$, we can strip it from the expression of $ll y_2$:
$
  ll y_2 = & y''_2 + p(x) y'_2 + q(x) y_2 = g(x) \
         = & underbrace(u [y''_1 + p(x) y'_1 + q(x) y_1], ll y_1 = 0) + [u'' y_1 + (2 u' y_1 + p y_1 u')] \
         = & u'' y_1 + (2 u' y_1 + p y_1 u').
$
Let $W = u', W'' = u''$,
$
  W' + ((2 y'_1) / y_1 + p(x)) W = & g(x) / (y_1 (x)).
$
Go back to using the integrating factor method,
$
                 F(x) = & e^(2 integral y'_1 / y_1 dd(x) + integral p(x) dd(x)) \
                      = & e^(ln(y_1^2) + integral p(x) dd(x)) \
                      = & y_1^2 e^(integral p(x) dd(x)) \
  dv(, x) [F(x) W(x)] = & (F(x) g(x)) / (y_1 (x)) \
            F(x) W(x) = & integral (F(x) g(x)) / (y_1 (x)) dd(x) + C_2 \
        W(x) = u' (x) = & 1 / F(x) integral (F(x) g(x)) / (y_1 (x)) dd(x) + C_2 / F(x) \
                 u(x) = & C_2 integral 1 / F(x) dd(x) \
                        & + limits(integral)^x 1 / F(s) [limits(integral)^s (F(t) g(t)) / (y_1 (t)) dd(t)] dd(s) \
                 y(x) = & C_1 y_1 (x) + u(x) y_1 (x).
$

#problem[
  Solve
  $
    ll y = & x^2 y'' + x y' - 9 y = 0.
  $

  #solution[
    Let $y = x^r$.
    $
                   ll x^r = & [a r (r - 1) + b r + c] x^r \
                            & = 0 \
      a r^2 + (b - a) r + c & = 0
    $
    where $a = 1, b = 1, c = -9$.
    $
         Delta = & (b - a)^2 - 4 a c \
               = & 36 > 0; \
      r_(1, 2) = & (plus.minus sqrt(Delta)) / 2 \
               = & plus.minus 3 \
          y(x) = & C_1 x^3 + C_2 x^(-3).
    $
  ]
]

#problem[
  Solve
  $
    ll y = & x^2 y'' + x y' + 9 y = 0.
  $

  #solution[
    Similar to the previous problem,
    $
      r_(1, 2) = & plus.minus 3 i \
          y(x) = & C_1 cos(3 ln(x)) + C_2 sin(3 ln(x)).
    $
  ]
]

#problem[
  Solve
  $
    ll y = & x^2 y'' - x y' + y = 0.
  $

  #solution[
    $Delta = 0, r = 1$.
    $
      y(x) = & C_1 x' + C_2 x' ln(x).
    $
  ]
]

= Laplace Transform
This is a transformation useful in _linear systems _only (for now).
The reasons are that it
+ works for discountinuous and impulsive forcing;
+ easier than undetermined coefficient method;

It looks like
$
               ll y = & g(t) \
          "something" & "transforms" \
  D(s) [Y(s) + I C] = & G(s) \
               Y(s) = & 1 / D(s) [G(s) - I C] \
          "something" & "transforms" \
               y(t) = & ...
$

#definition(title: [Laplace Transform])[
  Given a (at least piecewise) continuous function $f(t)$ defined on $[0, oo)$.
  The Laplace Transform is, $F(s)$, is then
  $
    F(s) = llt (f(t)) (s) = & integral_(0)^(oo) e^(-s t) f(t) dd(t)
  $
  where $s > 0 "and may be complex"$.

  It is a linear operation.
]

For transformation conclusions, a table of Laplace Transforms is given for this course.
Most importantly,
$
  llt (dv(, t, n) f(t)) = & s^n F(s) - f^(n - 1) (0) - s f^(n - 2) (0) - ... - s^(n - 1) f(0)
$
and
$
  llt (e^(i a t)) = & (s + i a) / (s^2 + a^2) \
   llt (sin(a t)) = & a / (s^2 + a^2) quad s > 0 \
   llt (cos(a t)) = & s / (s^2 + a^2) quad s > abs(a).
$
which can all be proven by induction.

== Forced Simple Harmonic Motion
#problem[
  Solve
  $
    dot.double(x) + omega_0^2 x = & F_0 / m cos(omega t) \
              x(0) = dot(x) (0) = & 0
  $
  where $omega eq.not omega_0 = sqrt(k / m)$.

  #solution[
    $
             llt (dot.double(x)) + omega_0^2 llt (x) = & F_0 / m llt (cos(omega t)) \
      s^2 X(s) - dot(x)(0) - s x(0) + omega_0^2 X(s) = & F_0 / m s / (s^2 + omega^2) \
                              (s^2 + omega_0^2) X(s) = & F_0 / m s / (s^2 + omega^2) \
                                                X(s) = & F_0 / m s / ((s^2 + omega^2) (s^2 + omega_0^2)).
    $
    Let
    $
      A / (s^2 + omega^2) + B / (s^2 + omega_0^2) = & 1 / ((s^2 + omega^2) (s^2 + omega_0^2)) \
          A (s^2 + omega^2) + B (s^2 + omega_0^2) = & 1.
    $
    Let $s = i omega$, $1 = B (omega_0^2 - omega^2)$, $B = 1 / (omega_0^2 - omega^0)$.
    Let $s = i omega_0$, $1 = A (-omega_0^2 + omega^2)$, $A = -1 / (omega_0^2 - omega^0)$.
    $
      X(s) = & F_0 / (m (omega_0^2 - omega^2)) [s / (s^2 + omega^2) - s / (s^2 + omega^2)].
    $

    Notice that some terms are result of Laplace Transforms, we linearly inverse them:
    $
      x(t) = & F_0 / (m (omega_0^2 - omega^2)) [cos(omega t) - cos(omega_0 t)].
    $
  ]
]

The linearity of Laplace Transform makes it applicable to compleicated differential equations.

#problem[
  Solve
  $
    y'' + 4y' + 5y = & t e^(-t) - 3e^(-t) + 4e^(-2t) \
      y(0) = y'(0) = & 0.
  $

  #solution[
    Apply Laplace Transform to boths sides:
    $
      [s^2 Y(s) - s y(0) - y(0)'] + 4 [s Y(s) - y(0)] + 5 Y(s) = & 1 / (s + 1)^2 - 3 / (s + 1) - 4 / (s + 2) \
      (s^2 + 4s + 5) Y(s) = & s^2 / ((s + 1)^2 + (s + 2)) \
      Y(s) = & s^2 / ((s^2 + 4s + 5) (s + 1)^2 + (s + 2)) \
      = & (A s + B) / (s^2 + 4s + 5) + C / (s + 1)^2 + E / (s + 1) + F / (s + 2).
    $

    How do we solve this gigantic partial fraction?
    We can certainly solve the system of five equations, but we will see the other method below.
    #align(right)[cont.]
  ]
]

== Partial Fraction Tricks
Given a fraction of $s$:
$
  Y(s) = & N(s) / D(s).
$

=== Contribution of an Unrepeated Factor
When there is a _unrepeated_ $(s - a)$ factor in the denominator, we can conclude that $D(a) = & 0$, hence
$
                       Y(s) = & A / (s - a) + "Rest" \
               (s - a) Y(s) = & ((s - a) N(s)) / (D(s) - D(a)) \
                            = & A + (s - a) "Rest" \
  lim_(s -> a) (s - a) Y(s) = & lim_(s -> a) (s - a) / (D(s) - D(a)) N(s) \
                            = & A \
             N(a) / (D'(a)) = & A.
$

=== Contribution of a Repeated Factor
When there is a _repeated_ $(s - a)$ factor of $m$ times in the denominator, we can factor each of it out:
$
  Y(s) = & A_m / (s - a)^m + A_(m - 1) / (s - a)^(m - 1) + A_(m - 2) / (s - a)^(m - 2) + ... + A_1 / (s - a) + "Rest".
$
Let $Q(s) = (s - a)^m Y(s)$.
$
          Q(s) = & A_m + A_(m - 1) (s - a) + A_(m - 2) (s - a)^2 + ... + A_1 (s - a)^(m - 1) + (s - a)^m "Rest" \
          Q(a) = & A_m \
         Q'(a) = & A_(m - 1) \
        Q''(a) = & 2 A_(m - 2) \
  Q^(m - 1)(a) = & (m - 1)! A_1.
$

=== Contribution of a Complex Conjugate Pair
When there is a complex conjugate pair, $(s - a) (s - conj(a))$ in the denominator, let $a = alpha + i beta, b = alpha - i beta$.
$
  (s - a) (s - conj(a)) = & s^2 - 2 alpha s + alpha^2 + beta^2 \
                        = & (s - alpha)^2 + beta^2.
$
The complex conjugate pair is likely a result of a quadratic expression, force it to zero to solve for $alpha, beta$.

#solution[
  #align(right)[cont.]
  $
    s^2 + 4s + 5 = & s^2 + 4s + 4 + 1 \
                 = & (s + 2)^2 + 1 \
            Y(s) = & (A s + B) / ((s + alpha)^2 + beta^2) + "Rest".
  $
  Let $R(s) = (s - a) (s - conj(a)) Y(s)$.
  $
        R(s) = & A s + B + ((s - a) (s - conj(a))) "Rest" \
        R(a) = & A a + B \
             = & A (alpha + i beta) + B \
             = & (A alpha + B) + A beta i \
    Re(R(a)) = & A alpha + B \
    Im(R(a)) = & A beta i.
  $
  // TODO
]

== Discontinuous and Impossible Loads
#definition(title: [Heaviside Step Function])[
  $u(t)$, or written $H(t)$, is a function of that
  $
    u(t - a) = & cases(
                   0 quad & t < a,
                   1 & t > a
                 )
  $
  where $a$ is a constant.
]

$
  llt (u(t - a)) = & integral_(0)^(oo) e^(-s t) u(t - a) dd(t) \
                 = & integral_(a)^(oo) e^(-s t) dd(t) \
                 = & e^(-a s) / s.
$
As expected (integration by parts), $llt (u'(t - a))$ gives one more $s$ and hence
$
  llt (u' (t - a)) = & e^(-a s).
$

Similarly, let $tau = t - a$,
$
  llt (u(t - a) f(t - a)) = & integral_(a)^(oo) e^(-s t) f(t - a) dd(t) \
                          = & integral_(0)^(oo) e^(-s (a + tau)) f(tau) dd(tau) \
                          = & e^(-a s) integral_(0)^(oo) e^(-s t) f(tau) dd(tau) \
                          = & e^(-a s) F(s).
$

#definition(title: [Dirac Delta Function])[
  Let
  $
    d_k (t) = & cases(
                  1 / (2k) quad & abs(t) < k,
                  0 & abs(k) > k
                ),
  $
  the Dirac Delta function is defined by
  $
    delta(t) = & lim_(k -> 0) d_k (t).
  $
]

This function, despite having no apparent "area under the curve", is integrated to be one.
$
  integral_(-oo)^oo delta(t) dd(t) = & lim_(k -> 0) integral_(-k)^(k) 1 / (2k) dd(t) = 1.
$
Given a continuous function $f(t)$,
$
  integral_(-oo)^(oo) delta(t - t_0) f(t) dd(t) = & lim_(k -> 0) integral_(-oo)^(oo) d_k (t - t_0) f(t) dd(t) \
                                                = & lim_(k -> 0) integral_(t_0 - k)^(t_0 + k) f(t) / (2k) dd(t) \
                                                = & lim_(k -> 0) f(t) integral_(t_0 - k)^(t_0 + k) dd(t) / (2k) \
                                                = & f(t_0).
$

== Convolution Theorem
#definition(title: [Laplace Convolution])[
  Let $f, g$ be functions defined on $[0, oo)$, then
  $
    (f convolve g)(t) := & integral_(0)^(t) f(tau) g(t - tau) dd(tau).
  $

  This operation is linear, hence
  $
               (f convolve g) (t) = & (g convolve f) (t) \
    h convolve (alpha f + beta g) = & alpha h convolve f + beta g convolve g.
  $
]

#theorem(title: [Convolution Theorem])[
  If $H(s) = F(s) G(s)$, then $h = f convolve g$, i.e. $llt (f convolve g) (s) = F(s) G(s)$.
]

#problem[
  Transform
  $
    g(t) = & cases(
               2t quad & 0 < t < 1,
               2 & 1 < t
             ).
  $

  #solution[
    Rewrite $g(t)$ so it is transformable.
    $
      g(t) = & 2t - 2t u(t - 1) + 2 u(t - 1) \
           = & 2t - 2 (t - 1) u(t - 1) - 2 u(t - 1) + 2 u(t - 1) \
           = & 2t - 2(t - 1) u(t - 1)
    $
    where $u$ is a Heaviside function.
    $
      llt(g(t))(s) = G(s) = & 2 / s^2 - 2 / s^2 e^(-s) \
                            & = 2 / s^2 (1 - e^(-s))
    $
  ]
]

#problem[
  $
    y'' + y = g(x) = & 2t - 2u(t - 1) (t - 1) \
          y(0) = 0 = & y'(0).
  $

  #solution[
    $
      llt (y'') + llt (y) = G(s) = & s^2 Y(s) - y'(0) - s y(0) + Y(s) \
                                 = & 2 / s^2 (1 - e^(-s)).
    $
    Notice that how this is equivalent to the convolution method.
    $
      (s^2 + 1) Y(s) = & G(s) \
                Y(s) = & 1 / (s^2 + 1) G(s) \
                     = & 2 / (s^2 (s^2 + 1)) (1 - e^(-s)).
    $

    #solution(title: [Partial Fraction Expansion])[
      $
        2 / (s^2 (s^2 + 1)) = & 1 / s^2 - 1 / (s^2 + 1) \
                       Y(s) = & 2 (1 / s^2 - 1 / (s^2 + 1)) - 2 e^(-s) (1 / s^2 - 1 / (s^2 + 1)) \
                       y(s) = & 2 (t - sin(t)) - 2 u(t - 1) ((t - 1) - sin(t - 1)).
      $
    ]

    #solution(title: [Convolution Theorem])[
      $
        y(t) = & integral_(0)^(t) g(t) sin(t - tau) dd(tau) \
        = & integral_(0)^(t) [2 tau - 2 (tau - 1) u(tau - 1)] sin(t - tau) dd(tau) \
        = & integral_(0)^(t) underbrace(2 tau sin(t - tau), I_1) - underbrace(2 (tau - 1) u(tau - 1) sin(t - tau), I_2) dd(tau) \
        I_1 = & 2 integral_(0)^(t) tau sin(t - tau) dd(tau) \
        = & 2 [evaluated(tau cos(t - tau))_0^t - integral_(0)^(t) cos(t - tau) dd(tau)] \
        = & 2 [(t - 0) + evaluated(sin(t - tau))_0^t] \
        = & 2 (t - sin(t)). \
        I_2 = & 2 integral_(0)^(t) (tau - 1) u(tau - 1) sin(t - tau) dd(tau) \
        = & ... \
        = & 2 [(t - 1) - sin(t - 1)].
      $
    ]
  ]

]

= System of Linear Equations
A first-order linear system looks like
$
  dv(vb(x), t) = & A(t) vb(x) (t) + vb(f) (t).
$

Any linear $n$th order ODE can be rewritten as a first-order linear system.
For each derivation of $x$, make a new $x_i$, so that each $x_i$ is at most order one.

Given a linear system, if $vb(x_1) (t), vb(x_2) (t)$ are solutions to the homogeneous equation
$
  vb(x)' = & A vb(x),
$
then so $vb(x) (t) = alpha vb(x_1) (t) + beta vb(x_2) (t)$.

Note how this is similar to solution to a second-order differential equation being the linear combination of two solutions.

#problem[
  Find the general solution of
  $
    x' = & -3x - 2y \
    y' = & -2x - 6y.
  $

  #solution[
    Let
    $
       vb(x) = & vec(x, y) \
           A = & mat(-3, -2; -2, -6) \
      vb(x)' = & A vb(x).
    $

    Similar to how we guessed the solutions of first-order equations, guess
    $
                              vb(x) (t) = & e^(r t) vb(v) \
                             vb(x)' (t) = & e^(r t) r vb(v) = A e^(r t) vb(v) = A vb(x) \
      e^(r t) r vb(v) - A e^(r t) vb(v) = & vb(0) \
                        (r I - A) vb(v) = & vb(0).
    $
    Looks familiar? $r$ is an eigenvalue for a nontrivial solution.
    Recall from MATH 152 that we do
    $
      det(r I - A) = & 0 \
                   = & mdet(r + 3, 2; 2, r + 6) \
               r_1 = & -7 \
               r_2 = & -2.
    $
    Use $r_1 = -7$,
    $
      mat(-4, 2; 2, -1) vec(v_1, v_2) = & vec(0, 0) \
                                  v_1 = & 1 \
                                  v_2 = & 2.
    $
    Use $r_2 = -2$,
    $
      mat(1, 2; 2, 4) vec(v_1, v_2) = & vec(0, 9) \
                                v_1 = & -2 \
                                v_2 = & 1.
    $

    The general solution is
    $
      vb(x) = & (t) c_1 e^(-7 t) vec(1, 2) + c_2 e^(-2 t) vec(-2, 1) \
      = & underbrace(mat(e^(-7 t), -2 e^(-2, t); 2 e^(-7 t), e^(-2 t)), #[$phi(t)$ \ the Fundamental Matrix]) vec(c_1, c_2).
    $

    #problem[
      Given
      $
        vb(x) (0) = & vec(5, 4),
      $
      find the particular solution.

      #solution[
        $
          vb(x) (t) = & phi(t) vb(c) \
          vb(x) (0) = & phi(0) vb(c) \
                    = & mat(1, -2; 2, 1) vec(c_1, c_2) \
                    = & vec(5, 4) \
                c_1 = & 13 / 5 \
                c_2 = & - 6 / 5.
        $

        #note-box[
          $
                vb(c) = & phi(0)^(-1) vb(x) (0) \
            vb(x) (t) = & underbrace(phi(t) phi(0)^(-1), #[the Flow Matrix]) vb(x) (0).
          $
        ]
      ]
    ]
  ]
]

